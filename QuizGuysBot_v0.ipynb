{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuizGuysBot v0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1r1gYIPvbS8"
      },
      "source": [
        "# Склеиваем все вместе?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZbHEnnSvf_2"
      },
      "source": [
        "pip install python-telegram-bot;pip install transformers;pip install sentencepiece;pip install flair; pip install python-docx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv6LDlKE6uX0"
      },
      "source": [
        "pip install git+https://github.com/boudinfl/pke.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfTE6eEowIRN"
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\r\n",
        "import json\r\n",
        "from flair.models import SequenceTagger\r\n",
        "from flair.data import Sentence\r\n",
        "\r\n",
        "import json\r\n",
        "from random import seed, randint\r\n",
        "from telegram import Update\r\n",
        "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext\r\n",
        "import re\r\n",
        "import random\r\n",
        "from nltk import pos_tag\r\n",
        "import nltk\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('universal_tagset')\r\n",
        "import pke"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uls1LTkhXNNu"
      },
      "source": [
        "We use a pretrained question generation model (https://huggingface.co, \"mrm8488/t5-base-finetuned-question-generation-ap\") and a pretrained NER model (flair)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYLqQASAwLTC"
      },
      "source": [
        "\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\r\n",
        "CGmodel = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\r\n",
        "NERmodel = SequenceTagger.load('ner-ontonotes-fast') #.load('ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwUWmvSFwMP-"
      },
      "source": [
        "def get_question(answer, context, max_length=64):\r\n",
        "  input_text = \"answer: %s  context: %s </s>\" % (answer, context)\r\n",
        "  features = tokenizer([input_text], return_tensors='pt')\r\n",
        "\r\n",
        "  output = CGmodel.generate(input_ids=features['input_ids'], \r\n",
        "               attention_mask=features['attention_mask'],\r\n",
        "               max_length=max_length)\r\n",
        "  #question = tokenizer.decode(output[0])[len('<pad> question: '):-1 * len('</s>')]question: \r\n",
        "  question = tokenizer.decode(output[0])[len('question: '):]\r\n",
        "  return question\r\n",
        "\r\n",
        "\r\n",
        "def do_NER(context):\r\n",
        "  s = Sentence(context)\r\n",
        "  NERmodel.predict(s)\r\n",
        "  raw = s.to_dict(tag_type='ner')\r\n",
        "  answers = []\r\n",
        "  for item in raw['entities']:\r\n",
        "      answers.append(item['text'])\r\n",
        "  if not answers:\r\n",
        "    answers = get_key_words(context)\r\n",
        "  return list(map(lambda x: x.capitalize(), list(set(map(lambda x: x.lower(), answers)))))\r\n",
        "\r\n",
        "\r\n",
        "def get_questions(answers, context):\r\n",
        "  generated_questions = []\r\n",
        "  for answer in answers:\r\n",
        "    #print('context:', context, '\\n', 'answer:', answer, '\\n', 'generated question:', get_question(answer, context))\r\n",
        "    generated_questions.append({'question':get_question(answer, context), 'answer':answer})\r\n",
        "  return generated_questions\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW_ylqnfh0Ra"
      },
      "source": [
        "def generate_files_by_message(text):\r\n",
        "  dict_of_questions = get_questions(do_NER(text), text)\r\n",
        "  return save_questions_into_docx_files(text, dict_of_questions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "970PwzKC8DMd"
      },
      "source": [
        "def get_key_words(text, num_nouns=3):\r\n",
        "  pos = {'NOUN', 'PROPN', 'ADJ'}\r\n",
        "\r\n",
        "  extractor = pke.unsupervised.SingleRank()\r\n",
        "\r\n",
        "  extractor.load_document(input=text,\r\n",
        "                          language='en',\r\n",
        "                          normalization=None)\r\n",
        "\r\n",
        "  extractor.candidate_selection(pos=pos)\r\n",
        "\r\n",
        "  extractor.candidate_weighting(window=1,\r\n",
        "                                pos=pos)\r\n",
        "\r\n",
        "  keyphrases = extractor.get_n_best(n=10)\r\n",
        "\r\n",
        "  return random.choices(list(map(lambda x: x[0], keyphrases)), k=num_nouns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXP02wLYecg9"
      },
      "source": [
        "from uuid import uuid1\r\n",
        "from docx import Document\r\n",
        "import os\r\n",
        "import re\r\n",
        "\r\n",
        "def get_human_readable_file_names(text, file_extantion = '.docx', num_tokens_in_filename=5):\r\n",
        "    tokens = [token for token in re.split('[^A-Za-z0-9]', text) if token]\r\n",
        "    normalized_beginning = '_'.join(tokens[:num_tokens_in_filename])\r\n",
        "    student_file_name = '_'.join([normalized_beginning, 'student']) + file_extantion\r\n",
        "    teacher_file_name = '_'.join([normalized_beginning, 'teacher']) + file_extantion\r\n",
        "    return student_file_name, teacher_file_name\r\n",
        "\r\n",
        "def get_unique_file_names(text, file_extantion = '.docx'):\r\n",
        "    file_id = uuid1().hex\r\n",
        "    student_file_name = '_'.join([file_id, 'student']) + file_extantion\r\n",
        "    teacher_file_name = '_'.join([file_id, 'teacher']) + file_extantion\r\n",
        "    return student_file_name, teacher_file_name\r\n",
        "\r\n",
        "\r\n",
        "def save_questions_into_docx_files(text:str, questions:list, upload_folder=''):\r\n",
        "    '''\r\n",
        "    Функция принимает текст и вопросы к нему, формирует и сохраняет два документа .docx (l)\r\n",
        "    и возвращает пути к ним\r\n",
        "    \r\n",
        "    questions - данные в формате\r\n",
        "    [\r\n",
        "      { \r\n",
        "        'answer': 'the last few years',\r\n",
        "        'question': 'When did Deep Learning pick up pace in academia and industry?'\r\n",
        "      },\r\n",
        "     {\r\n",
        "       'answer': 'Machine Learning',\r\n",
        "        'question': 'What type of practitioners have infiltrated the academia to its roots?'\r\n",
        "     }\r\n",
        "    ]\r\n",
        "    '''\r\n",
        "    student_docx_name, teacher_docx_name = get_unique_file_names(text)\r\n",
        "    student_docx_path = os.path.join(upload_folder, student_docx_name)\r\n",
        "    teacher_docx_path = os.path.join(upload_folder, teacher_docx_name)\r\n",
        "    student_document = Document()\r\n",
        "    teacher_document = Document()\r\n",
        "    for document in [student_document, teacher_document]:\r\n",
        "        document.add_heading('Text', 1)\r\n",
        "        paragraphs = [paragraph for paragraph in text.split('\\n') if paragraph]\r\n",
        "        for paragraph in paragraphs:\r\n",
        "            document.add_paragraph(paragraph)\r\n",
        "    student_document.add_heading('Questions', 1)\r\n",
        "    teacher_document.add_heading('Questions with answers', 1)\r\n",
        "    for question_id, question_data in enumerate(questions):\r\n",
        "        question = question_data['question']\r\n",
        "        answer = question_data['answer']\r\n",
        "        enumerated_question = str(question_id+1) + '. ' + question\r\n",
        "        enumerated_question_with_answer = enumerated_question + '\\nAnswer: ' + answer \r\n",
        "        student_document.add_paragraph(enumerated_question)\r\n",
        "        teacher_document.add_paragraph(enumerated_question_with_answer)\r\n",
        "    student_document.save(student_docx_path)\r\n",
        "    teacher_document.save(teacher_docx_path)\r\n",
        "    return student_docx_path, teacher_docx_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUkTyoLvweSY",
        "outputId": "96d2f328-e791-4305-c868-8b50495c39ae"
      },
      "source": [
        "\"\"\"\r\n",
        "Simple Bot to reply to Telegram messages.\r\n",
        "First, a few handler functions are defined. Then, those functions are passed to\r\n",
        "the Dispatcher and registered at their respective places.\r\n",
        "Then, the bot is started and runs until we press Ctrl-C on the command line.\r\n",
        "Usage:\r\n",
        "Basic Echobot example, repeats messages.\r\n",
        "Press Ctrl-C on the command line or send a signal to the process to stop the\r\n",
        "bot.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "with open('token.txt', 'r') as txt:\r\n",
        "  for line in txt:\r\n",
        "    token = line\r\n",
        "\r\n",
        "# Define a few command handlers. These usually take the two arguments update and\r\n",
        "# context. Error handlers also receive the raised TelegramError object in error.\r\n",
        "def start(update: Update, context: CallbackContext) -> None:\r\n",
        "    \"\"\"Send a message when the command /start is issued.\"\"\"\r\n",
        "    start_mes = '''Привет, я QuizGuy, чтобы составить тест в автоматическом режиме просто пришли мне текст (На английском!) в следующем сообщении!\r\n",
        "    '''\r\n",
        "    update.message.reply_text(start_mes)\r\n",
        "\r\n",
        "def help(update: Update, context: CallbackContext) -> None:\r\n",
        "    \"\"\"Send a message when the command /start is issued.\"\"\"\r\n",
        "    help_mes = '''Чтобы сгенерировать тест в автоматическом режиме, просто пришли мне текст (на английском) в следующем сообщении.\r\n",
        "    '''\r\n",
        "    update.message.reply_text(help_mes)\r\n",
        "\r\n",
        "\r\n",
        "def chat(update: Update, context: CallbackContext) -> None:\r\n",
        "    \"\"\"Chat with user \"\"\"\r\n",
        "    \"\"\"\r\n",
        "    # Logging user request for development purposes \r\n",
        "    print(f\"Handling message  {update.message.chat_id}-{update.message.message_id} from {update.message.chat.username}\")\r\n",
        "    # Log messages to file\r\n",
        "    with open(f\"log/{update.message.chat_id}-{update.message.message_id}.log\", \"a\", encoding=\"utf-8\") as f:\r\n",
        "        f.write(f\",\\n{update.message.to_json()}\")\r\n",
        "    \"\"\"\r\n",
        "    # Pass user message to bot_reply for processing \r\n",
        "    grateful_list = ['Спасибо, что воспользовались нашим ботом!', 'Будем рады составить для вас еще одно задание по тексту!', 'Надеемся, что наши вопросы вам понравятся.']\r\n",
        "    user_mes = update.message.text\r\n",
        "    context.bot.send_message(chat_id=update.effective_chat.id, text='Начинаю обработку нейросетями...')\r\n",
        "    student_docx_path, teacher_docx_path = generate_files_by_message(user_mes)\r\n",
        "    student_human_readable_filename, teacher_human_readable_filename = get_human_readable_file_names(user_mes)\r\n",
        "    context.bot.send_message(chat_id=update.effective_chat.id, text='Подготавливаю файлы...')\r\n",
        "    context.bot.send_document(chat_id=update.effective_chat.id, document=open(student_docx_path, 'rb'), filename=student_human_readable_filename)\r\n",
        "    context.bot.send_document(chat_id=update.effective_chat.id, document=open(teacher_docx_path, 'rb'), filename=teacher_human_readable_filename)\r\n",
        "    context.bot.send_message(chat_id=update.effective_chat.id, text=random.choice(grateful_list))\r\n",
        "\r\n",
        "def main():\r\n",
        "    \"\"\"Start the bot.\"\"\"\r\n",
        "    # Create the Updater and pass it your bot's token.\r\n",
        "    # Make sure to set use_context=True to use the new context based callbacks\r\n",
        "    # Post version 12 this will no longer be necessary\r\n",
        "    updater = Updater(token, use_context=True)\r\n",
        "\r\n",
        "    # Get the dispatcher to register handlers\r\n",
        "    dispatcher = updater.dispatcher\r\n",
        "\r\n",
        "    # on different commands - answer in Telegram\r\n",
        "    dispatcher.add_handler(CommandHandler(\"start\", start))\r\n",
        "    dispatcher.add_handler(CommandHandler(\"help\", help))\r\n",
        "    # on noncommand i.e message - echo the message on Telegram\r\n",
        "    dispatcher.add_handler(MessageHandler(Filters.text & ~Filters.command, chat))\r\n",
        "\r\n",
        "    # Start the Bot\r\n",
        "    updater.start_polling()\r\n",
        "\r\n",
        "    # Run the bot until you press Ctrl-C or the process receives SIGINT,\r\n",
        "    # SIGTERM or SIGABRT. This should be used most of the time, since\r\n",
        "    # start_polling() is non-blocking and will stop the bot gracefully.\r\n",
        "    updater.idle()\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_t5.py:184: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_t5.py:184: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_t5.py:184: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
            "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzfKfh0Ux0iY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}